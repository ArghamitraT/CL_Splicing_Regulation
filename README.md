# CLADES
Contrastive Learning Augmented DifferEntial Splicing with Orthologous Positive Pairs

### 1. Installation

Clone the repository and create the CLADES environment:

```bash
git clone https://github.com/ArghamitraT/CLADES.git
cd CLADES

conda env create -n clades_env -f environment.yml
conda activate clades_env

pip install -e .
```

### 2. Set up Weights & Biases (W&B)
Before running any training script, edit the following Hydra config files:

- `configs/pretrain_CLADES.yaml`
- `configs/finetune_CLADES.yaml`

```yaml
wandb:
  api_key: "NEEDED"
```
and the replacement:

```yaml
wandb:
  api_key: "<YOUR_WANDB_API_KEY>"
```

### 3. Pre-training

- Option A â€” Bash 
```bash
cd scripts
bash pretrain_CLADES.sh
```

- Option B â€” Python
```bash
cd scripts
python pretrain_CLADES.py
```
The following Hydra overrides are commonly adjusted during CLADES contrastive pretraining:

- **task=introns_cl** â€” activates CLADES contrastive learning pipeline  
- **embedder="mtsplice"** â€” MTSplice-style dual-branch encoder  
- **tokenizer="onehot_tokenizer"** â€” one-hot intron boundary tokenization  
- **loss="supcon"** â€” supervised contrastive loss  
- **dataset.n_augmentations** â€” number of positive species views (default = 2)  
- **trainer.max_epochs** â€” total pretraining epochs  
- **task.global_batch_size** â€” global batch size across GPUs  
- **optimizer.lr** â€” contrastive learning learning rate  
- **dataset.min_views** â€” required number of species views per exon (default â‰¥ 30)  
- **dataset.fivep_ovrhang**, **dataset.threep_ovrhang** â€” intron window sizes  

For full details, see: `scripts/pretrain_CLADES.sh`


### 4. Fine-tuning

- Option A â€” Bash 
```bash
cd scripts
bash finetune_CLADES.sh
```

- Option B â€” Python
```bash
cd scripts
python finetune_CLADES.py
```

Important configurable parameters:

- **aux_models.mtsplice_weights** â€” path to pretrained CLADES encoder
- **aux_models.eval_weights** â€” checkpoint for evaluation-only mode
- **aux_models.freeze_encoder** â€” freeze encoder (true) or fine-tune (false)
- **aux_models.warm_start** â€” initialize from pretrained encoder
- **optimizer.lr** â€” learning rate
- **trainer.max_epochs** â€” number of epochs
- **dataset.fivep_ovrhang, dataset.threep_ovrhang** â€” intron window sizes

For full details, see: `scripts/finetune_CLADES.sh`

### ğŸ“‚ Sample data

`data/` folder contains sample data for pre-training and finetuning


### ğŸ“‚ Output Organization

All training runs create timestamped directories under `output/`, for example:

```bash
output/
â”œâ”€â”€ pretrain_2025_11_14_23_12_22/
â””â”€â”€ finetune_2025_11_14_23_46_21/
```

Each run contains:

```bash
output/<run_name>/
â”œâ”€â”€ hydra/ # Hydra config snapshots
â”œâ”€â”€ wandb/ # Weights & Biases logs
â””â”€â”€ checkpoints/ # Model checkpoints
```

### ğŸ—‚ï¸ Configuration Layout
```bash
configs/
 â”œâ”€â”€ aux_models/       # Pretrained model weights, MTSplice settings, eval model paths
 â”œâ”€â”€ callbacks/        # Lightning callbacks (checkpointing, early stopping, LR schedulers)
 â”œâ”€â”€ dataset/          # Dataset parameters (paths, window sizes, species settings)
 â”œâ”€â”€ embedder/         # Encoder architecture configs (MTSplice, ResNet, TISFM, etc.)
 â”œâ”€â”€ loss/             # Loss function configs (SupCon, NT-Xent, BCE, KL)
 â”œâ”€â”€ model/            # Full model assembly (encoder + projection head)
 â”œâ”€â”€ optimizer/        # Optimizer and scheduler settings
 â”œâ”€â”€ task/             # Task definitions (contrastive training, PSI regression)
 â”œâ”€â”€ tokenizer/        # Sequence tokenization settings (one-hot, k-mer, etc.)
 â”œâ”€â”€ trainer/          # Lightning Trainer config (devices, precision, epochs)
 â”œâ”€â”€ pretrain_CLADES.yaml    # Main config for contrastive pretraining
 â””â”€â”€ finetune_CLADES.yaml    # Main config for PSI regression fine-tuning


scripts/
 â”œâ”€â”€ pretrain_CLADES.sh   # SLURM/bash wrapper for contrastive pretraining
 â”œâ”€â”€ finetune_CLADES.sh   # SLURM/bash wrapper for PSI regression fine-tuning
 â”œâ”€â”€ pretrain_CLADES.py   # Python entry point for contrastive pretraining
 â””â”€â”€ finetune_CLADES.py   # Python entry point for PSI regression fine-tuning

 src/
 â”œâ”€â”€ datasets/          # Dataset classes for contrastive pretraining & PSI regression
 â”‚    â”œâ”€â”€ base.py               # Abstract dataset + shared utilities
 â”‚    â”œâ”€â”€ introns_alignment.py  # Loads cross-species intron alignment data
 â”‚    â”œâ”€â”€ auxiliary_jobs.py     # Helper preprocessing utilities
 â”‚    â””â”€â”€ lit.py                # LightningDataModule
 â”‚
 â”œâ”€â”€ embedder/          # Encoders (MTSplice, ResNet-style, TISFM, etc.)
 â”‚    â”œâ”€â”€ base.py
 â”‚    â”œâ”€â”€ utils.py
 â”‚    â””â”€â”€ mtsplice/
 â”‚
 â”œâ”€â”€ loss/              # Loss functions (contrastive + regression)
 â”‚    â”œâ”€â”€ MTSPLiceBCELoss.py
 â”‚    â””â”€â”€ supcon.py
 â”‚
 â”œâ”€â”€ model/             # LightningModule model definitions
 â”‚    â”œâ”€â”€ lit.py
 â”‚    â”œâ”€â”€ simclr.py
 â”‚    â””â”€â”€ MTSpliceBCE.py
 â”‚
 â”œâ”€â”€ tokenizers/        # Sequence tokenizers
 â”‚    â””â”€â”€ onehot_tokenizer.py
 â”‚
 â”œâ”€â”€ trainer/           # Trainer-level utilities
 â”‚    â””â”€â”€ utils.py
 â”‚
 â””â”€â”€ utils/             # Global project utilities
      â”œâ”€â”€ config.py
      â””â”€â”€ utils.py

```
